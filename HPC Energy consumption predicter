import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
import torch  
import torch.nn as nn  
import torch.optim as optim  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split  
import warnings  

# Suppress font-related warnings  
warnings.filterwarnings("ignore", category=UserWarning)  

# 1. Data Preparation  
def create_sample_data():  
    """Create sample dataset if file doesn't exist"""  
    data = pd.DataFrame({  
        'Year': [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023],  
        'Global_Computing_Power': [1000, 1200, 1500, 2000, 2800, 4000, 6000, 9000, 14000, 22000, 35000, 55000, 85000, 130000],  
        'HPC_Energy_Consumption': [4.511, 12.516, 20.385, 29.630, 38.823, 47.458, 59.000, 76.000, 114.000, 148.000, 158.000, 200.000, 234.000, 248.000],  
        'Renewable_Energy_Proportion': [26.45, 28.0, 29.5, 31.0, 32.5, 34.0, 35.5, 37.0, 38.5, 40.0, 41.5, 43.0, 44.5, 46.0],  
        'Carbon_Emissions': [83600000, 114400000, 160500000, 229400000, 332400000, 486400000, 716500000, 1060300000, 1574300000, 2342483383, 3500000000, 5200000000, 7700000000, 11400000000]  
    })  
    data.to_csv("global_hpc_energy_data.csv", index=False)  
    return data  

try:  
    data = pd.read_csv("global_hpc_energy_data.csv")  
except FileNotFoundError:  
    print("Data file not found, creating sample data...")  
    data = create_sample_data()  

print("\nOriginal data preview:")  
print(data.head())  

# 2. Feature Engineering and Splitting  
X = data[["Year", "Global_Computing_Power", "Renewable_Energy_Proportion", "Carbon_Emissions"]].values  
y = data["HPC_Energy_Consumption"].values.reshape(-1, 1)  

# Time-series split (no shuffling)  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  

# 3. Proper Scaling Implementation  
scaler_X = StandardScaler()  
scaler_y = StandardScaler()  

# Fit only on training data  
X_train_scaled = scaler_X.fit_transform(X_train)  
y_train_scaled = scaler_y.fit_transform(y_train)  

# Transform test data with training parameters  
X_test_scaled = scaler_X.transform(X_test)  
y_test_scaled = scaler_y.transform(y_test)  

# Convert to PyTorch tensors  
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)  
y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)  
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)  
y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)  

class HPCEnergyPredictor(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),

            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),

            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.2),

            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)
# Initialize model  
model = HPCEnergyPredictor()  

# 5. Training Setup  
criterion = nn.MSELoss()  
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.5)  

# 6. Validation Split (from training data)  
X_train_full, X_val, y_train_full, y_val = train_test_split(  
    X_train_tensor, y_train_tensor, test_size=0.2, shuffle=False)  

# 7. Training Loop with Early Stopping  
num_epochs = 3000  
train_losses = []  
val_losses = []  
best_val_loss = float('inf')  
patience = 100  
patience_counter = 0  

print("\nStarting training...")  
for epoch in range(num_epochs):  
    # Training  
    model.train()  
    optimizer.zero_grad()  
    outputs = model(X_train_full)  
    loss = criterion(outputs, y_train_full)  
    loss.backward()  
    optimizer.step()  
    
    # Validation  
    model.eval()  
    with torch.no_grad():  
        val_outputs = model(X_val)  
        val_loss = criterion(val_outputs, y_val)  
    
    # Learning rate scheduling  
    scheduler.step(val_loss)  
    
    # Record losses  
    train_losses.append(loss.item())  
    val_losses.append(val_loss.item())  
    
    # Check for improvement  
    if val_loss.item() < best_val_loss:  
        best_val_loss = val_loss.item()  
        patience_counter = 0  
        torch.save(model.state_dict(), 'best_model.pth')  
    else:  
        patience_counter += 1  
    
    # Print progress  
    if (epoch + 1) % 100 == 0:  
        print(f"Epoch [{epoch+1}/{num_epochs}] - "  
              f"Train Loss: {loss.item():.6f} - "  
              f"Val Loss: {val_loss.item():.6f} - "  
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")  
    
    # Early stopping  
    if patience_counter >= patience:  
        print(f"\nEarly stopping at epoch {epoch+1}")  
        break  

# 8. Load Best Model  
model.load_state_dict(torch.load('best_model.pth'))  

# 9. Evaluation  
model.eval()  
with torch.no_grad():  
    test_outputs = model(X_test_tensor)  
    test_loss = criterion(test_outputs, y_test_tensor)  
    
    # Inverse transform for original scale metrics  
    test_preds = scaler_y.inverse_transform(test_outputs.numpy())  
    test_true = scaler_y.inverse_transform(y_test_tensor.numpy())  
    
    mse = test_loss.item()  
    mae = np.mean(np.abs(test_preds - test_true))  
    r2 = 1 - (np.sum((test_true - test_preds)**2) /   
              np.sum((test_true - np.mean(test_true))**2))  

print("\nFinal Evaluation:")  
print(f"Test MSE: {mse:.6f}")  
print(f"Test MAE (original scale): {mae:.6f}")  
print(f"R-squared: {r2:.6f}")  

# 10. Visualization  
plt.figure(figsize=(18, 6))  

# Loss curves  
plt.subplot(1, 3, 1)  
plt.plot(train_losses, label='Training Loss')  
plt.plot(val_losses, label='Validation Loss')  
plt.xlabel('Epochs')  
plt.ylabel('Loss (MSE)')  
plt.title('Training Progress')  
plt.legend()  
plt.grid(True)  


plt.subplot(1, 3, 2)
plt.scatter(test_true, test_preds, alpha=0.6, color='blue', label='Predictions')
plt.plot([min(test_true), max(test_true)], 
         [min(test_true), max(test_true)], 
         'r--', lw=2, label='Perfect Fit')
plt.xlabel('Actual HPC Energy Consumption')
plt.ylabel('Predicted HPC Energy Consumption')
plt.title('Actual vs Predicted Values')
plt.legend()
plt.grid(True)

# Time Series Comparison
plt.subplot(1, 3, 3)
years = data['Year'].values
all_X_scaled = scaler_X.transform(X)
with torch.no_grad():
    all_preds_scaled = model(torch.tensor(all_X_scaled, dtype=torch.float32))
all_preds = scaler_y.inverse_transform(all_preds_scaled.numpy())

plt.plot(years, data['HPC_Energy_Consumption'], 'bo-', label='Actual')
plt.plot(years, all_preds, 'r--', label='Predicted')
plt.xlabel('Year')
plt.ylabel('Energy Consumption')
plt.title('HPC Energy Consumption Over Time')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('hpc_energy_results.png', dpi=300, bbox_inches='tight')
plt.show()

# 11. Future Prediction Example
def predict_future(year):
    """Predict energy consumption for a future year"""
    if year <= 2023:
        print("Warning: Year should be after 2023 for future prediction")
    
    # Use median values for other features
    input_features = np.array([
        year,
        np.median(data['Global_Computing_Power']),
        np.median(data['Renewable_Energy_Proportion']),
        np.median(data['Carbon_Emissions'])
    ]).reshape(1, -1)
    
    input_scaled = scaler_X.transform(input_features)
    with torch.no_grad():
        pred = model(torch.tensor(input_scaled, dtype=torch.float32))
    
    return scaler_y.inverse_transform(pred.numpy())[0][0]

# Example prediction
try:
    future_year = int(input("\nEnter a future year for prediction (>=2024): "))
    prediction = predict_future(future_year)
    print(f"Predicted HPC energy consumption for {future_year}: {prediction:.2f}")
except ValueError:
    print("Invalid year input")
